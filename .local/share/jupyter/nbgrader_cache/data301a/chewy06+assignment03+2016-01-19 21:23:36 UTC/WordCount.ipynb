{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {}
   },
   "source": [
    "# Word Counting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook introduces some of the basic tools and idea for working with natural language (text), including tokenization and word counting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {}
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {}
   },
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PUNCTUATION = '`~!@#$%^&*()_-+={[}]|\\:;\"<,>.?/}\\t\\n'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a generator function, `remove_punctuation`, that removes punctuation from an iterator of words and yields the cleaned words:\n",
    "\n",
    "* Strip the punctuation characters at the beginning and end of each word.\n",
    "* Replace `-` by a space if found in the middle of the word and split on that white space to yield multiple words.\n",
    "* If a word is all punctuation, don't yield it at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gen(words, punctuation):\n",
    "    a = list(punctuation)\n",
    "    iter_words = iter(words)\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            word = next(iter_words)\n",
    "        except StopIteration:\n",
    "            break\n",
    "        else:\n",
    "            for next_ in a:\n",
    "                while len(word) != 0 and next_ == word[0]:\n",
    "                    word = word[1::]\n",
    "                while len(word) != 0 and next_ == word[len(word)-1]:\n",
    "                    word = word[:len(word)-1:]\n",
    "            if len(word) != 0:\n",
    "                word = list(word)\n",
    "                for index, item in enumerate(word):\n",
    "                    if item == '-':\n",
    "                        word[index] = ' '\n",
    "                        \n",
    "                word = ''.join(word)\n",
    "                word = word.split(' ')   \n",
    "                for item in word:\n",
    "                    yield item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "00fb6bf5d41ed46a831d90886eabfb41",
     "grade": false,
     "grade_id": "wordcounta",
     "locked": false,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def remove_punctuation(words, punctuation=PUNCTUATION):\n",
    "    vals = gen(words, punctuation)\n",
    "    return vals\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object gen at 0x7f439839b948>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_punctuation(['!data;'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'generator'>\n"
     ]
    }
   ],
   "source": [
    "print(type(remove_punctuation(['!!'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(isinstance(remove_punctuation(['!!']), types.GeneratorType))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "dbb2caad06a60b143118c978d9cc5914",
     "grade": true,
     "grade_id": "wordcountb",
     "locked": true,
     "points": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert list(remove_punctuation(['!data;']))==['data']\n",
    "assert list(remove_punctuation(['!data-science:']))==['data', 'science']\n",
    "assert list(remove_punctuation(['!!']))==[]\n",
    "assert isinstance(remove_punctuation(['!!']), types.GeneratorType)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a generator function, `lower_words`, that makes each word in an iterator lowercase, yielding each lowercase word:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def to_lower(words):\n",
    "    iter_words = iter(words)\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            word = next(iter_words)\n",
    "        except:\n",
    "            break\n",
    "        else:\n",
    "            word = list(word)\n",
    "            for index, item in enumerate(word):\n",
    "                word[index] = item.lower()\n",
    "                \n",
    "            word = ''.join(word)\n",
    "            yield word\n",
    "                \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "checksum": "9daa0db11a4ae562fcae3d5e443c8496",
     "grade": false,
     "grade_id": "wordcountc",
     "locked": false,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def lower_words(words):\n",
    "    words = to_lower(words)\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "d02af789287a63f22712f8ccace94b8c",
     "grade": true,
     "grade_id": "wordcountd",
     "locked": true,
     "points": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert isinstance(lower_words('AAA'), types.GeneratorType)\n",
    "assert list(lower_words('This IS NOT LoWerCaSe'.split(' ')))==['this', 'is', 'not', 'lowercase']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Stop words](https://en.wikipedia.org/wiki/Stop_words) are common words in text that are typically filtered out when performing natural language processing. Typical stop words are *and*, *of*, *a*, *the*, etc.\n",
    "\n",
    "Write a generator function, `remove_stop_words`, that removes stop words from an iterator, yielding the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type([1,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_words(words, stop):\n",
    "    iter_word = iter(words)\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            word = next(iter_word)\n",
    "        except:\n",
    "            break\n",
    "        else:\n",
    "            if stop:\n",
    "                if type(stop) != list:\n",
    "                    stop = stop.split(' ')\n",
    "                    \n",
    "                if word not in stop:\n",
    "                    yield word\n",
    "            else:\n",
    "                yield word\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "checksum": "63a85260f81b1521bb6867cf6f87108f",
     "grade": false,
     "grade_id": "wordcounte",
     "locked": false,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def remove_stop_words(words, stop_words=None):\n",
    "    vals = remove_words(words, stop_words)\n",
    "    return vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "83fb7f89e1a718459e86632f46c7aefd",
     "grade": true,
     "grade_id": "wordcountf",
     "locked": true,
     "points": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert list(remove_stop_words('the begin to the end a of the day'.split(' '), stop_words='a the')) == \\\n",
    "    ['begin', 'to', 'end', 'of', 'day']\n",
    "assert list(remove_stop_words('the begin to the end a of the day'.split(' '), stop_words=['a', 'the'])) == \\\n",
    "    ['begin', 'to', 'end', 'of', 'day']\n",
    "assert list(remove_stop_words('the begin to the end a of the day'.split(' '))) == \\\n",
    "    ['the', 'begin', 'to', 'the', 'end', 'a', 'of', 'the', 'day']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Tokenization](https://en.wikipedia.org/wiki/Lexical_analysis#Tokenization) is the process of taking a string or line of text and returning a sequence of words, or *tokens*, with the following transforms applied\n",
    "\n",
    "* Punctuation removed\n",
    "* All words lowercased\n",
    "* Stop words removed\n",
    "\n",
    "Write a generator function, `tokenize_line`, that yields tokenized words from a an input line of text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "c3393ebf28c23de9346413b59f3bad5b",
     "grade": false,
     "grade_id": "wordcountg",
     "locked": false,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def tokenize_line(line, stop_words=None, punctuation=PUNCTUATION):\n",
    "    rev_punc = remove_punctuation(line.split(' '))\n",
    "    lower_case = lower_words(list(rev_punc))\n",
    "    rev_stop = remove_stop_words(list(lower_case), stop_words)\n",
    "    return rev_stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "1810910521defcef31a581e750ff0846",
     "grade": true,
     "grade_id": "wordcounth",
     "locked": true,
     "points": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert isinstance(tokenize_line(\"This, is the way; that things will end\"), types.GeneratorType)\n",
    "assert list(tokenize_line(\"This, is the way; that things will end\", stop_words=['the', 'is'])) == \\\n",
    "    ['this', 'way', 'that', 'things', 'will', 'end']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a generator function, `tokenize_lines`, that can yield the tokens in an iterator of lines of text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def token_lines(lines, stop, punc):\n",
    "    iter_lines = iter(lines)\n",
    "    while True:\n",
    "        try:\n",
    "            line = next(iter_lines)\n",
    "        except:\n",
    "            break\n",
    "        else:\n",
    "            tokens = tokenize_line(line, stop, punc)\n",
    "            for token in tokens:\n",
    "                yield token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "checksum": "99351f2172da2d7a98069b5f3cb2b593",
     "grade": false,
     "grade_id": "wordcounti",
     "locked": false,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def tokenize_lines(lines, stop_words=None, punctuation=PUNCTUATION):\n",
    "    token = token_lines(lines, stop_words, punctuation)\n",
    "    return token\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "f5158101498f195d7d1ff6e317517719",
     "grade": true,
     "grade_id": "wordcountj",
     "points": 2
    }
   },
   "outputs": [],
   "source": [
    "wasteland = \"\"\"\n",
    "APRIL is the cruellest month, breeding\n",
    "Lilacs out of the dead land, mixing\n",
    "Memory and desire, stirring\n",
    "Dull roots with spring rain.\n",
    "\"\"\"\n",
    "\n",
    "assert isinstance(tokenize_lines(wasteland.splitlines()), types.GeneratorType)\n",
    "\n",
    "assert list(tokenize_lines(wasteland.splitlines(), stop_words='is the of and')) == \\\n",
    "    ['april','cruellest','month','breeding','lilacs','out','dead','land',\n",
    "     'mixing','memory','desire','stirring','dull','roots','with','spring',\n",
    "     'rain']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Counting words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {}
   },
   "source": [
    "Write a function, `count_words`, that takes an iterator of words and returns a dictionary where the keys in the dictionary are the unique words in the list and the values are the word counts. Be careful to not ever assume that the input iterator is a concrete list/tuple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def counting_words(words):\n",
    "    curr_dict = {}\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            word = next(words)\n",
    "        except:\n",
    "            break\n",
    "        else:\n",
    "            if word in curr_dict:\n",
    "                curr_dict[word] += 1\n",
    "            else:\n",
    "                curr_dict[word] = 1;\n",
    "                \n",
    "    return curr_dict\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "checksum": "ea8c4caf9bc8768b7120758339168c4b",
     "grade": false,
     "grade_id": "wordcountk",
     "locked": false,
     "points": 2,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def count_words(words):\n",
    "    mydict = counting_words(words)\n",
    "    return mydict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "65626ee77dd3656c2c7769f92a708e3e",
     "grade": true,
     "grade_id": "wordcountl",
     "points": 2
    }
   },
   "outputs": [],
   "source": [
    "assert count_words(tokenize_line('This, and The-this from, and A a a')) == \\\n",
    "    {'a': 3, 'and': 2, 'from': 1, 'the': 1, 'this': 2}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {}
   },
   "source": [
    "Write a function, `sort_word_counts`, that return a list of sorted word counts:\n",
    "\n",
    "* Each element of the list should be a `(word, count)` tuple.\n",
    "* The list should be sorted by the word counts, with the higest counts coming first.\n",
    "* To perform this sort, look at using the `sorted` function.\n",
    "\n",
    "This can return a concrete list as the memory here is proportional to the number of unique words in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sort_dict(dic):\n",
    "    new_dict = {}\n",
    "    mydict_vals = sorted(dic.values())\n",
    "    mydict_items = sorted(dic, key=dic.__getitem__)\n",
    "    mydict = mydict_vals[::-1]\n",
    "    mydict_items = mydict_items[::-1]\n",
    "    \n",
    "    items = iter(mydict_items)\n",
    "    vals = iter(mydict)\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            val = next(vals)\n",
    "            item = next(items)\n",
    "        except:\n",
    "            break\n",
    "        else:\n",
    "            yield item, val\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('a', 3), ('and', 2), ('from', 1), ('the', 1), ('this', 2)}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(sort_dict(count_words(tokenize_line('This, and The-this from, and A a a'))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "checksum": "7f3df7bad47e7944b6ebe42edb56734a",
     "grade": false,
     "grade_id": "wordcountm",
     "locked": false,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def sort_word_counts(wc):\n",
    "    mydict = sort_dict(wc)\n",
    "    return mydict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "5a713da40b47c9f7acc3399d04737a54",
     "grade": true,
     "grade_id": "wordcountn",
     "points": 2
    }
   },
   "outputs": [],
   "source": [
    "assert set(sort_word_counts(count_words(tokenize_line('This, and The-this from, and A a a')))) == \\\n",
    "    {('a', 3), ('and', 2), ('this', 2), ('the', 1), ('from', 1)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File IO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a generator function, `files_to_lines`, that takes an iterator of filenames, and yields the lines in all of those files. Make sure to not ever create a concrete list/tuple in this process to keep your memory consumption $\\mathcal{O}(1)$. Make sure you use a `with` statement to properly close each file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def gen_files(files):\n",
    "    iter_files = iter(files)\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            file = next(iter_files)\n",
    "        except:\n",
    "            break\n",
    "        else:\n",
    "            with open(file, 'r') as f:\n",
    "                iter_lines = iter(f.readline())\n",
    "            while True:\n",
    "                try:\n",
    "                    line = next(iter_lines)\n",
    "                except:\n",
    "                    f.close()\n",
    "                    break\n",
    "                else:\n",
    "                    yield line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "checksum": "3dc19a25969ce3e61799353de5dd3892",
     "grade": false,
     "grade_id": "wordcounto",
     "locked": false,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def files_to_lines(files):\n",
    "    lines = gen_files(files)\n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting file1.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile file1.txt\n",
    "This is the first line in the first file.\n",
    "This is the secon line in the first file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting file2.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile file2.txt\n",
    "This is the first line in the second file.\n",
    "This is the second line in the second file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['T',\n",
       " 'h',\n",
       " 'i',\n",
       " 's',\n",
       " ' ',\n",
       " 'i',\n",
       " 's',\n",
       " ' ',\n",
       " 't',\n",
       " 'h',\n",
       " 'e',\n",
       " ' ',\n",
       " 'f',\n",
       " 'i',\n",
       " 'r',\n",
       " 's',\n",
       " 't',\n",
       " ' ',\n",
       " 'l',\n",
       " 'i',\n",
       " 'n',\n",
       " 'e',\n",
       " ' ',\n",
       " 'i',\n",
       " 'n',\n",
       " ' ',\n",
       " 't',\n",
       " 'h',\n",
       " 'e',\n",
       " ' ',\n",
       " 'f',\n",
       " 'i',\n",
       " 'r',\n",
       " 's',\n",
       " 't',\n",
       " ' ',\n",
       " 'f',\n",
       " 'i',\n",
       " 'l',\n",
       " 'e',\n",
       " '.',\n",
       " '\\n',\n",
       " 'T',\n",
       " 'h',\n",
       " 'i',\n",
       " 's',\n",
       " ' ',\n",
       " 'i',\n",
       " 's',\n",
       " ' ',\n",
       " 't',\n",
       " 'h',\n",
       " 'e',\n",
       " ' ',\n",
       " 'f',\n",
       " 'i',\n",
       " 'r',\n",
       " 's',\n",
       " 't',\n",
       " ' ',\n",
       " 'l',\n",
       " 'i',\n",
       " 'n',\n",
       " 'e',\n",
       " ' ',\n",
       " 'i',\n",
       " 'n',\n",
       " ' ',\n",
       " 't',\n",
       " 'h',\n",
       " 'e',\n",
       " ' ',\n",
       " 's',\n",
       " 'e',\n",
       " 'c',\n",
       " 'o',\n",
       " 'n',\n",
       " 'd',\n",
       " ' ',\n",
       " 'f',\n",
       " 'i',\n",
       " 'l',\n",
       " 'e',\n",
       " '.',\n",
       " '\\n']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(files_to_lines(['file1.txt', 'file2.txt']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "de95cb8efb1b8fbfaf409e4e9d5cf39e",
     "grade": true,
     "grade_id": "wordcountp",
     "locked": true,
     "points": 2,
     "solution": false
    }
   },
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-51-a9c5aa858b24>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m      \u001b[1;34m'This is the secon line in the first file.'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m      \u001b[1;34m'This is the first line in the second file.\\n'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m      'This is the second line in the second file.']\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "assert isinstance(files_to_lines(['file1.txt', 'file2.txt']), types.GeneratorType)\n",
    "assert list(files_to_lines(['file1.txt', 'file2.txt'])) == \\\n",
    "    ['This is the first line in the first file.\\n',\n",
    "     'This is the secon line in the first file.',\n",
    "     'This is the first line in the second file.\\n',\n",
    "     'This is the second line in the second file.']\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All together now"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now use all of the above functions to perform tokenization and word counting for all of the text documents described by your instructor:\n",
    "\n",
    "* You should be able to perform this in a memory efficient manner.\n",
    "* Read your stop words from the included `stopwords.txt` file.\n",
    "* Save your sorted word counts to a variable named `swc`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "61d3f1d15631b5e5f306ff6cefb7a6a9",
     "grade": false,
     "grade_id": "wordcountq",
     "locked": false,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "with open('stopwords.txt', 'r') as f:\n",
    "    data = f.read()\n",
    "swc = sort_word_counts(count_words(tokenize_line(data.splitlines())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "checksum": "22bee56e22c6472ff4788273297a621c",
     "grade": true,
     "grade_id": "wordcountr",
     "locked": true,
     "points": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert [word for word, count in swc[0:10]] == \\\n",
    "    ['said', 'one', 'mr', 'now', 'upon', 'will', 'little', 'time', 'man', 'like']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a horizontal bar chart for the top 50 words using text and simple calls to `print`:\n",
    "\n",
    "* For each word, encode the count as a bar of `*` characters.\n",
    "* You will have to scale the length of your bars to fit on the page.\n",
    "* Provide labels for each bar that indicates which word the counts apply to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "e1eb080305e87b369745977473b12992",
     "grade": true,
     "grade_id": "wordcounts",
     "locked": false,
     "points": 2,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
